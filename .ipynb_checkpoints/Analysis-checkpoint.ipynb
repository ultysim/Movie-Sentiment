{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "MAX_WORDS from 20000 to 10000 hurt accuracy and lead to overfitting with weighted embedding LSTM benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as k\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import GRU, Dense, Activation, Embedding, Input, Dropout, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.tsv',sep='\\t')\n",
    "test = pd.read_csv('test.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17780 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#Create tokens and sequences with those tokens\n",
    "MAX_WORDS = 20000\n",
    "\n",
    "texts = data['Phrase'].values\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "tokenizer.fit_on_texts(test['Phrase'].values)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pad the sequences to the max sequence length\n",
    "MAX_SEQUENCE_LENGTH = max([len(i) for i in sequences])\n",
    "x = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y = np_utils.to_categorical(data['Sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(data['Sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create embedding matrix for the tokens\n",
    "GRU_UNITS = 128\n",
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,mask_zero=True))\n",
    "model.add(GRU(GRU_UNITS))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/3\n",
      "124848/124848 [==============================] - 574s - loss: 0.9098 - acc: 0.6284 - val_loss: 0.9601 - val_acc: 0.6062\n",
      "Epoch 2/3\n",
      "124848/124848 [==============================] - 571s - loss: 0.7318 - acc: 0.6958 - val_loss: 0.9676 - val_acc: 0.6098\n",
      "Epoch 3/3\n",
      "124848/124848 [==============================] - 572s - loss: 0.6504 - acc: 0.7269 - val_loss: 0.9979 - val_acc: 0.6038\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x, y, batch_size=32, epochs=3, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Thoughts:\n",
    "\n",
    "Balance data set by removing a large portion of neutral reviews\n",
    "\n",
    "Use pretrained encoding matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/3\n",
      "124848/124848 [==============================] - 714s - loss: 0.9383 - acc: 0.6219 - val_loss: 0.9704 - val_acc: 0.6070\n",
      "Epoch 2/3\n",
      "124848/124848 [==============================] - 752s - loss: 0.7478 - acc: 0.6926 - val_loss: 0.9734 - val_acc: 0.6100\n",
      "Epoch 3/3\n",
      "124848/124848 [==============================] - 978s - loss: 0.6654 - acc: 0.7218 - val_loss: 0.9874 - val_acc: 0.6089\n"
     ]
    }
   ],
   "source": [
    "#Create embedding matrix for the tokens\n",
    "GRU_UNITS = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,mask_zero=True))\n",
    "model.add(LSTM(GRU_UNITS,recurrent_dropout=0.2,dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x, y, batch_size=32, epochs=3, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/3\n",
      "124848/124848 [==============================] - 879s - loss: 0.9053 - acc: 0.6287 - val_loss: 0.9102 - val_acc: 0.6222\n",
      "Epoch 2/3\n",
      "124848/124848 [==============================] - 858s - loss: 0.7504 - acc: 0.6871 - val_loss: 0.9153 - val_acc: 0.6253\n",
      "Epoch 3/3\n",
      "124848/124848 [==============================] - 865s - loss: 0.6870 - acc: 0.7124 - val_loss: 0.9356 - val_acc: 0.6182\n"
     ]
    }
   ],
   "source": [
    "#Create embedding matrix for the tokens\n",
    "GRU_UNITS = 256\n",
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,mask_zero=True))\n",
    "model.add(LSTM(GRU_UNITS,recurrent_dropout=0.2,dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x, y, batch_size=32, epochs=2, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/10\n",
      "124848/124848 [==============================] - 838s - loss: 0.9902 - acc: 0.5927 - val_loss: 0.9514 - val_acc: 0.6009\n",
      "Epoch 2/10\n",
      "124848/124848 [==============================] - 837s - loss: 0.9049 - acc: 0.6234 - val_loss: 0.9269 - val_acc: 0.6108\n",
      "Epoch 3/10\n",
      "124848/124848 [==============================] - 863s - loss: 0.8581 - acc: 0.6433 - val_loss: 0.9205 - val_acc: 0.6145\n",
      "Epoch 4/10\n",
      "124848/124848 [==============================] - 847s - loss: 0.8195 - acc: 0.6588 - val_loss: 0.9254 - val_acc: 0.6152\n",
      "Epoch 5/10\n",
      "124848/124848 [==============================] - 842s - loss: 0.7917 - acc: 0.6703 - val_loss: 0.9262 - val_acc: 0.6201\n",
      "Epoch 6/10\n",
      "124848/124848 [==============================] - 836s - loss: 0.7703 - acc: 0.6790 - val_loss: 0.9320 - val_acc: 0.6156\n",
      "Epoch 7/10\n",
      "124848/124848 [==============================] - 833s - loss: 0.7521 - acc: 0.6887 - val_loss: 0.9299 - val_acc: 0.6161\n",
      "Epoch 8/10\n",
      "124848/124848 [==============================] - 805s - loss: 0.7386 - acc: 0.6927 - val_loss: 0.9336 - val_acc: 0.6198\n",
      "Epoch 9/10\n",
      "124848/124848 [==============================] - 861s - loss: 0.7258 - acc: 0.6989 - val_loss: 0.9369 - val_acc: 0.6172\n",
      "Epoch 10/10\n",
      "124848/124848 [==============================] - 963s - loss: 0.7149 - acc: 0.7022 - val_loss: 0.9517 - val_acc: 0.6213\n"
     ]
    }
   ],
   "source": [
    "#Create embedding matrix for the tokens\n",
    "GRU_UNITS = 256\n",
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,mask_zero=True,trainable=False))\n",
    "model.add(LSTM(GRU_UNITS,recurrent_dropout=0.2,dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x, y, batch_size=32, epochs=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#May be some training left to do here, try for more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/3\n",
      "124848/124848 [==============================] - 789s - loss: 0.9163 - acc: 0.6217 - val_loss: 0.9118 - val_acc: 0.6237\n",
      "Epoch 2/3\n",
      "124848/124848 [==============================] - 677s - loss: 0.7678 - acc: 0.6798 - val_loss: 0.9140 - val_acc: 0.6215\n",
      "Epoch 3/3\n",
      "124848/124848 [==============================] - 814s - loss: 0.7086 - acc: 0.7051 - val_loss: 0.9279 - val_acc: 0.6192\n"
     ]
    }
   ],
   "source": [
    "#Create embedding matrix for the tokens\n",
    "GRU_UNITS = 256\n",
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,mask_zero=True))\n",
    "model.add(GRU(GRU_UNITS,recurrent_dropout=0.2,dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x, y, batch_size=32, epochs=3, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17780 unique tokens.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Try with less words:\n",
    "MAX_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "texts = data['Phrase'].values\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "tokenizer.fit_on_texts(test['Phrase'].values)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "#Pad the sequences to the max sequence length\n",
    "MAX_SEQUENCE_LENGTH = max([len(i) for i in sequences])\n",
    "x = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y = np_utils.to_categorical(data['Sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO try decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/10\n",
      "124848/124848 [==============================] - 71s - loss: 0.9264 - acc: 0.5785 - val_loss: 0.9556 - val_acc: 0.5694\n",
      "Epoch 2/10\n",
      "124848/124848 [==============================] - 71s - loss: 0.8025 - acc: 0.6238 - val_loss: 0.9527 - val_acc: 0.5873\n",
      "Epoch 3/10\n",
      "124848/124848 [==============================] - 70s - loss: 0.7510 - acc: 0.6422 - val_loss: 1.0193 - val_acc: 0.5912\n",
      "Epoch 4/10\n",
      "124848/124848 [==============================] - 69s - loss: 0.7161 - acc: 0.6583 - val_loss: 0.9935 - val_acc: 0.5967\n",
      "Epoch 5/10\n",
      "124848/124848 [==============================] - 69s - loss: 0.6853 - acc: 0.6732 - val_loss: 0.9923 - val_acc: 0.5958\n",
      "Epoch 6/10\n",
      "124848/124848 [==============================] - 71s - loss: 0.6476 - acc: 0.7043 - val_loss: 1.0480 - val_acc: 0.5826\n",
      "Epoch 7/10\n",
      "124848/124848 [==============================] - 70s - loss: 0.6105 - acc: 0.7230 - val_loss: 1.0606 - val_acc: 0.5911\n",
      "Epoch 8/10\n",
      "124848/124848 [==============================] - 71s - loss: 0.5853 - acc: 0.7361 - val_loss: 1.1671 - val_acc: 0.5799\n",
      "Epoch 9/10\n",
      "124848/124848 [==============================] - 74s - loss: 0.5627 - acc: 0.7452 - val_loss: 1.1470 - val_acc: 0.5780\n",
      "Epoch 10/10\n",
      "124848/124848 [==============================] - 73s - loss: 0.5426 - acc: 0.7546 - val_loss: 1.1618 - val_acc: 0.5640\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(256,3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(256,3,activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5, activation = 'sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x, y, batch_size=32, epochs=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/2\n",
      " 37632/124848 [========>.....................] - ETA: 520s - loss: 1.0224 - acc: 0.5861"
     ]
    }
   ],
   "source": [
    "# BEST MODEL\n",
    "UNITS = 256\n",
    "EMBEDDING_DIM = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,mask_zero=True))\n",
    "model.add(LSTM(UNITS,recurrent_dropout=0.2,dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x, y, batch_size=32, epochs=2, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test model:\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(test['Phrase'].values)\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "with tf.device('/gpu:0'):\n",
    "    out = model.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
